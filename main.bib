@article{vaswani2017attention,
  title   = {Attention is all you need},
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}
@inproceedings{lee2019set,
  title        = {Set transformer: A framework for attention-based permutation-invariant neural networks},
  author       = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
  booktitle    = {International conference on machine learning},
  pages        = {3744--3753},
  year         = {2019},
  organization = {PMLR}
}
@inproceedings{chan2022data,
  title     = {Data distributional properties drive emergent in-context learning in transformers},
  author    = {Chan, Stephanie CY and Santoro, Adam and Lampinen, Andrew Kyle and Wang, Jane X and Singh, Aaditya K and Richemond, Pierre Harvey and McClelland, James and Hill, Felix},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2022}
}
@article{brown2020language,
  title   = {Language models are few-shot learners},
  author  = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal = {Advances in neural information processing systems},
  volume  = {33},
  pages   = {1877--1901},
  year    = {2020}
}
@inproceedings{santoro2016meta,
  title        = {Meta-learning with memory-augmented neural networks},
  author       = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
  booktitle    = {International conference on machine learning},
  pages        = {1842--1850},
  year         = {2016},
  organization = {PMLR}
}
@inproceedings{hochreiter2001learning,
  title        = {Learning to learn using gradient descent},
  author       = {Hochreiter, Sepp and Younger, A Steven and Conwell, Peter R},
  booktitle    = {Artificial Neural Networks—ICANN 2001: International Conference Vienna, Austria, August 21--25, 2001 Proceedings 11},
  pages        = {87--94},
  year         = {2001},
  organization = {Springer}
}
@inproceedings{bellemare2017distributional,
  title        = {A distributional perspective on reinforcement learning},
  author       = {Bellemare, Marc G and Dabney, Will and Munos, R{\'e}mi},
  booktitle    = {International conference on machine learning},
  pages        = {449--458},
  year         = {2017},
  organization = {PMLR}
}
@book{sutton2018reinforcement,
  title     = {Reinforcement learning: An introduction},
  author    = {Sutton, Richard S and Barto, Andrew G},
  year      = {2018},
  publisher = {MIT press}
}
@article{wang2021alchemy,
  title   = {Alchemy: A benchmark and analysis toolkit for meta-reinforcement learning agents},
  author  = {Wang, Jane X and King, Michael and Porcel, Nicolas and Kurth-Nelson, Zeb and Zhu, Tina and Deck, Charlie and Choy, Peter and Cassin, Mary and Reynolds, Malcolm and Song, Francis and others},
  journal = {arXiv preprint arXiv:2102.02926},
  year    = {2021}
}
@article{greenwade93,
  author  = {George D. Greenwade},
  title   = {The {C}omprehensive {T}ex {A}rchive {N}etwork ({CTAN})},
  year    = {1993},
  journal = {TUGBoat},
  volume  = {14},
  number  = {3},
  pages   = {342--351}
}
@article{brooks2022context,
  title   = {In-Context Policy Iteration},
  author  = {Brooks, Ethan and Walls, Logan and Lewis, Richard L and Singh, Satinder},
  journal = {arXiv preprint arXiv:2210.03821},
  year    = {2022}
}
@article{chen2021evaluating,
  title   = {Evaluating large language models trained on code},
  author  = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal = {arXiv preprint arXiv:2107.03374},
  year    = {2021}
}
@article{freeman2021brax,
  title   = {Brax--A Differentiable Physics Engine for Large Scale Rigid Body Simulation},
  author  = {Freeman, C Daniel and Frey, Erik and Raichuk, Anton and Girgin, Sertan and Mordatch, Igor and Bachem, Olivier},
  journal = {arXiv preprint arXiv:2106.13281},
  year    = {2021}
}
@article{laskin2022context,
  title   = {In-context reinforcement learning with algorithm distillation},
  author  = {Laskin, Michael and Wang, Luyu and Oh, Junhyuk and Parisotto, Emilio and Spencer, Stephen and Steigerwald, Richie and Strouse, DJ and Hansen, Steven and Filos, Angelos and Brooks, Ethan and others},
  journal = {arXiv preprint arXiv:2210.14215},
  year    = {2022}
}
@misc{chen_relation_2022,
  title     = {On the {Relation} between {Sensitivity} and {Accuracy} in {In}-context {Learning}},
  url       = {http://arxiv.org/abs/2209.07661},
  abstract  = {In-context learning (ICL) suffers from oversensitivity to the prompt, which makes it unreliable in real-world scenarios. We study the sensitivity of ICL with respect to multiple types of perturbations. First, we find that label bias obscures true ICL sensitivity, and hence prior work may have significantly underestimated the true ICL sensitivity. Second, we observe a strong negative correlation between ICL sensitivity and accuracy, with sensitive predictions less likely to be correct. Motivated by these observations, we propose {\textbackslash}textsc\{SenSel\}, a few-shot selective prediction method based on ICL sensitivity. Experiments on ten classification benchmarks show that {\textbackslash}textsc\{SenSel\} consistently outperforms a commonly used confidence-based selective prediction baseline.},
  urldate   = {2022-09-28},
  publisher = {arXiv},
  author    = {Chen, Yanda and Zhao, Chen and Yu, Zhou and McKeown, Kathleen and He, He},
  month     = sep,
  year      = {2022},
  note      = {arXiv:2209.07661 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@misc{duan_rl2_2016,
  title      = {{RL}^2: {Fast} {Reinforcement} {Learning} via {Slow} {Reinforcement} {Learning}},
  shorttitle = {{RL}^2},
  url        = {http://arxiv.org/abs/1611.02779},
  doi        = {10.48550/arXiv.1611.02779},
  abstract   = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL\${\textasciicircum}2\$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL\${\textasciicircum}2\$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL\${\textasciicircum}2\$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL\${\textasciicircum}2\$ on a vision-based navigation task and show that it scales up to high-dimensional problems.},
  urldate    = {2022-08-11},
  publisher  = {arXiv},
  author     = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
  month      = nov,
  year       = {2016},
  note       = {arXiv:1611.02779 [cs, stat]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
  annote     = {Comment: 14 pages. Under review as a conference paper at ICLR 2017},
  file       = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/JET46HYH/Duan et al. - 2016 - RL\$^2\$ Fast Reinforcement Learning via Slow Reinf.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/E39WF2KB/1611.html:text/html}
}

@misc{nye_show_2021,
  title      = {Show {Your} {Work}: {Scratchpads} for {Intermediate} {Computation} with {Language} {Models}},
  shorttitle = {Show {Your} {Work}},
  url        = {http://arxiv.org/abs/2112.00114},
  doi        = {10.48550/arXiv.2112.00114},
  abstract   = {Large pre-trained language models perform remarkably well on tasks that can be done "in one pass", such as generating realistic text or synthesizing computer programs. However, they struggle with tasks that require unbounded multi-step computation, such as adding integers or executing programs. Surprisingly, we find that these same models are able to perform complex multi-step computations -- even in the few-shot regime -- when asked to perform the operation "step by step", showing the results of intermediate computations. In particular, we train transformers to perform multi-step computations by asking them to emit intermediate computation steps into a "scratchpad". On a series of increasingly complex tasks ranging from long addition to the execution of arbitrary programs, we show that scratchpads dramatically improve the ability of language models to perform multi-step computations.},
  urldate    = {2022-08-11},
  publisher  = {arXiv},
  author     = {Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and Sutton, Charles and Odena, Augustus},
  month      = nov,
  year       = {2021},
  note       = {arXiv:2112.00114 [cs]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
  file       = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/A4JLGPGP/Nye et al. - 2021 - Show Your Work Scratchpads for Intermediate Compu.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/MVCTCAXZ/2112.html:text/html}
}

@misc{wei_emergent_2022,
  title     = {Emergent {Abilities} of {Large} {Language} {Models}},
  url       = {http://arxiv.org/abs/2206.07682},
  doi       = {10.48550/arXiv.2206.07682},
  abstract  = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
  urldate   = {2022-08-11},
  publisher = {arXiv},
  author    = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  month     = jun,
  year      = {2022},
  note      = {arXiv:2206.07682 [cs]},
  keywords  = {Computer Science - Computation and Language},
  file      = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/L845TP2T/Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/F3DZ22Y6/2206.html:text/html}
}

@misc{noauthor_emergent_nodate,
  title   = {emergent abilities of large language models - {Google} {Search}},
  url     = {https://www.google.com/search?q=emergent+abilities+of+large+language+models&oq=Emergent+Abilities+of+Large+Language+Models&aqs=chrome.0.0i512j0i390l4.186j0j4&sourceid=chrome&ie=UTF-8},
  urldate = {2022-08-11}
}

@misc{chan_data_2022,
  title     = {Data {Distributional} {Properties} {Drive} {Emergent} {In}-{Context} {Learning} in {Transformers}},
  url       = {http://arxiv.org/abs/2205.05055},
  doi       = {10.48550/arXiv.2205.05055},
  abstract  = {Large transformer-based models are able to perform in-context few-shot learning, without being explicitly trained for it. This observation raises the question: what aspects of the training regime lead to this emergent behavior? Here, we show that this behavior is driven by the distributions of the training data itself. In-context learning emerges when the training data exhibits particular distributional properties such as burstiness (items appear in clusters rather than being uniformly distributed over time) and having large numbers of rarely occurring classes. In-context learning also emerges more strongly when item meanings or interpretations are dynamic rather than fixed. These properties are exemplified by natural language, but are also inherent to naturalistic data in a wide range of other domains. They also depart significantly from the uniform, i.i.d. training distributions typically used for standard supervised learning. In our initial experiments, we found that in-context learning traded off against more conventional weight-based learning, and models were unable to achieve both simultaneously. However, our later experiments uncovered that the two modes of learning could co-exist in a single model when it was trained on data following a skewed Zipfian distribution -- another common property of naturalistic data, including language. In further experiments, we found that naturalistic data distributions were only able to elicit in-context learning in transformers, and not in recurrent models. In sum, our findings indicate how the transformer architecture works together with particular properties of the training data to drive the intriguing emergent in-context learning behaviour of large language models, and how future work might encourage both in-context and in-weights learning in domains beyond language.},
  urldate   = {2022-08-11},
  publisher = {arXiv},
  author    = {Chan, Stephanie C. Y. and Santoro, Adam and Lampinen, Andrew K. and Wang, Jane X. and Singh, Aaditya and Richemond, Pierre H. and McClelland, Jay and Hill, Felix},
  month     = may,
  year      = {2022},
  note      = {arXiv:2205.05055 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
  file      = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/N45UJ89C/Chan et al. - 2022 - Data Distributional Properties Drive Emergent In-C.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/5Q62P9AV/2205.html:text/html}
}

@misc{vaswani_attention_2017,
  title     = {Attention {Is} {All} {You} {Need}},
  url       = {http://arxiv.org/abs/1706.03762},
  doi       = {10.48550/arXiv.1706.03762},
  abstract  = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  urldate   = {2022-08-11},
  publisher = {arXiv},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  month     = dec,
  year      = {2017},
  note      = {arXiv:1706.03762 [cs]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  annote    = {Comment: 15 pages, 5 figures},
  file      = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/B22DKGPZ/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/795D8RC9/1706.html:text/html}
}

@misc{finn_model-agnostic_2017,
  title     = {Model-{Agnostic} {Meta}-{Learning} for {Fast} {Adaptation} of {Deep} {Networks}},
  url       = {http://arxiv.org/abs/1703.03400},
  doi       = {10.48550/arXiv.1703.03400},
  abstract  = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  urldate   = {2022-08-11},
  publisher = {arXiv},
  author    = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  month     = jul,
  year      = {2017},
  note      = {arXiv:1703.03400 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
  annote    = {Comment: ICML 2017. Code at https://github.com/cbfinn/maml, Videos of RL results at https://sites.google.com/view/maml, Blog post at http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/},
  file      = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/93W55BG6/Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation o.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/APJSF62V/1703.html:text/html}
}

@inproceedings{hochreiter_learning_2001,
  address   = {Berlin, Heidelberg},
  series    = {Lecture {Notes} in {Computer} {Science}},
  title     = {Learning to {Learn} {Using} {Gradient} {Descent}},
  isbn      = {978-3-540-44668-2},
  doi       = {10.1007/3-540-44668-0_13},
  abstract  = {This paper introduces the application of gradient descent methods to meta-learning. The concept of “meta-learning”, i.e. of a system that improves or discovers a learning algorithm, has been of interest in machine learning for decades because of its appealing applications. Previous meta-learning approaches have been based on evolutionary methods and, therefore, have been restricted to small models with few free parameters. We make meta-learning in large systems feasible by using recurrent neural networks with their attendant learning routines as meta-learning systems. Our system derived complex well performing learning algorithms from scratch. In this paper we also show that our approach performs non-stationary time series prediction.},
  language  = {en},
  booktitle = {Artificial {Neural} {Networks} — {ICANN} 2001},
  publisher = {Springer},
  author    = {Hochreiter, Sepp and Younger, A. Steven and Conwell, Peter R.},
  editor    = {Dorffner, Georg and Bischof, Horst and Hornik, Kurt},
  year      = {2001},
  keywords  = {Boolean Function, Gradient Descent, Hide Layer, Learning Algorithm, Turing Machine},
  pages     = {87--94},
  file      = {Full Text PDF:/Users/ethan/Zotero/storage/NQECFMKS/Hochreiter et al. - 2001 - Learning to Learn Using Gradient Descent.pdf:application/pdf}
}

@misc{reed_generalist_2022,
  title     = {A {Generalist} {Agent}},
  url       = {http://arxiv.org/abs/2205.06175},
  doi       = {10.48550/arXiv.2205.06175},
  abstract  = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
  urldate   = {2022-08-09},
  publisher = {arXiv},
  author    = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and de Freitas, Nando},
  month     = may,
  year      = {2022},
  note      = {arXiv:2205.06175 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics},
  file      = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/PFN4FBCE/Reed et al. - 2022 - A Generalist Agent.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/WZIE7MIS/2205.html:text/html}
}

@misc{baker_video_2022,
  title      = {Video {PreTraining} ({VPT}): {Learning} to {Act} by {Watching} {Unlabeled} {Online} {Videos}},
  shorttitle = {Video {PreTraining} ({VPT})},
  url        = {http://arxiv.org/abs/2206.11795},
  doi        = {10.48550/arXiv.2206.11795},
  abstract   = {Pretraining on noisy, internet-scale datasets has been heavily studied as a technique for training models with broad, general capabilities for text, images, and other modalities. However, for many sequential decision domains such as robotics, video games, and computer use, publicly available data does not contain the labels required to train behavioral priors in the same way. We extend the internet-scale pretraining paradigm to sequential decision domains through semi-supervised imitation learning wherein agents learn to act by watching online unlabeled videos. Specifically, we show that with a small amount of labeled data we can train an inverse dynamics model accurate enough to label a huge unlabeled source of online data -- here, online videos of people playing Minecraft -- from which we can then train a general behavioral prior. Despite using the native human interface (mouse and keyboard at 20Hz), we show that this behavioral prior has nontrivial zero-shot capabilities and that it can be fine-tuned, with both imitation learning and reinforcement learning, to hard-exploration tasks that are impossible to learn from scratch via reinforcement learning. For many tasks our models exhibit human-level performance, and we are the first to report computer agents that can craft diamond tools, which can take proficient humans upwards of 20 minutes (24,000 environment actions) of gameplay to accomplish.},
  urldate    = {2022-08-09},
  publisher  = {arXiv},
  author     = {Baker, Bowen and Akkaya, Ilge and Zhokhov, Peter and Huizinga, Joost and Tang, Jie and Ecoffet, Adrien and Houghton, Brandon and Sampedro, Raul and Clune, Jeff},
  month      = jun,
  year       = {2022},
  note       = {arXiv:2206.11795 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  file       = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/Z32T7RSJ/Baker et al. - 2022 - Video PreTraining (VPT) Learning to Act by Watchi.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/3JQSRMUF/2206.html:text/html}
}

@misc{janner_offline_2021,
  title     = {Offline {Reinforcement} {Learning} as {One} {Big} {Sequence} {Modeling} {Problem}},
  url       = {http://arxiv.org/abs/2106.02039},
  doi       = {10.48550/arXiv.2106.02039},
  abstract  = {Reinforcement learning (RL) is typically concerned with estimating stationary policies or single-step models, leveraging the Markov property to factorize problems in time. However, we can also view RL as a generic sequence modeling problem, with the goal being to produce a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide effective solutions to the RL problem. To this end, we explore how RL can be tackled with the tools of sequence modeling, using a Transformer architecture to model distributions over trajectories and repurposing beam search as a planning algorithm. Framing RL as sequence modeling problem simplifies a range of design decisions, allowing us to dispense with many of the components common in offline RL algorithms. We demonstrate the flexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL. Further, we show that this approach can be combined with existing model-free algorithms to yield a state-of-the-art planner in sparse-reward, long-horizon tasks.},
  urldate   = {2022-08-09},
  publisher = {arXiv},
  author    = {Janner, Michael and Li, Qiyang and Levine, Sergey},
  month     = nov,
  year      = {2021},
  note      = {arXiv:2106.02039 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  annote    = {Comment: NeurIPS 2021 (spotlight). Project page and code at: https://trajectory-transformer.github.io/},
  file      = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/VI9DJDXD/Janner et al. - 2021 - Offline Reinforcement Learning as One Big Sequence.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/4SXRLIAG/2106.html:text/html}
}

@inproceedings{karimi_mahabadi_prompt-free_2022,
  address   = {Dublin, Ireland},
  title     = {Prompt-free and {Efficient} {Few}-shot {Learning} with {Language} {Models}},
  url       = {https://aclanthology.org/2022.acl-long.254},
  doi       = {10.18653/v1/2022.acl-long.254},
  abstract  = {Current methods for few-shot fine-tuning of pretrained masked language models (PLMs) require carefully engineered prompts and verbalizers for each new task to convert examples into a cloze-format that the PLM can score. In this work, we propose Perfect, a simple and efficient method for few-shot fine-tuning of PLMs without relying on any such handcrafting, which is highly effective given as few as 32 data points. Perfect makes two key design choices: First, we show that manually engineered task prompts can be replaced with task-specific adapters that enable sample-efficient fine-tuning and reduce memory and storage costs by roughly factors of 5 and 100, respectively. Second, instead of using handcrafted verbalizers, we learn new multi-token label embeddings during fine-tuning, which are not tied to the model vocabulary and which allow us to avoid complex auto-regressive decoding. These embeddings are not only learnable from limited data but also enable nearly 100x faster training and inference. Experiments on a wide range of few shot NLP tasks demonstrate that Perfect, while being simple and efficient, also outperforms existing state-of-the-art few-shot learning methods. Our code is publicly available at https://github.com/rabeehk/perfect.},
  urldate   = {2022-08-09},
  booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
  publisher = {Association for Computational Linguistics},
  author    = {Karimi Mahabadi, Rabeeh and Zettlemoyer, Luke and Henderson, James and Mathias, Lambert and Saeidi, Marzieh and Stoyanov, Veselin and Yazdani, Majid},
  month     = may,
  year      = {2022},
  pages     = {3638--3652},
  file      = {Full Text PDF:/Users/ethan/Zotero/storage/Z3SDSAR9/Karimi Mahabadi et al. - 2022 - Prompt-free and Efficient Few-shot Learning with L.pdf:application/pdf}
}

@misc{singh_know_2022,
  title      = {Know your audience: specializing grounded language models with the game of {Dixit}},
  shorttitle = {Know your audience},
  url        = {http://arxiv.org/abs/2206.08349},
  doi        = {10.48550/arXiv.2206.08349},
  abstract   = {Effective communication requires adapting to the idiosyncratic common ground shared with each communicative partner. We study a particularly challenging instantiation of this problem: the popular game Dixit. We formulate a round of Dixit as a multi-agent image reference game where a (trained) speaker model is rewarded for describing a target image such that one (pretrained) listener model can correctly identify it from a pool of distractors, but another listener cannot. To adapt to this setting, the speaker must exploit differences in the common ground it shares with the different listeners. We show that finetuning an attention-based adapter between a CLIP vision encoder and a large language model in this contrastive, multi-agent setting gives rise to context-dependent natural language specialization from rewards only, without direct supervision. In a series of controlled experiments, we show that the speaker can adapt according to the idiosyncratic strengths and weaknesses of various pairs of different listeners. Furthermore, we show zero-shot transfer of the speaker's specialization to unseen real-world data. Our experiments offer a step towards adaptive communication in complex multi-partner settings and highlight the interesting research challenges posed by games like Dixit. We hope that our work will inspire creative new approaches to adapting pretrained models.},
  urldate    = {2022-08-09},
  publisher  = {arXiv},
  author     = {Singh, Aaditya K. and Ding, David and Saxe, Andrew and Hill, Felix and Lampinen, Andrew K.},
  month      = jun,
  year       = {2022},
  note       = {arXiv:2206.08349 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
  annote     = {Comment: 27 pages, 9 figures},
  file       = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/CEV2KEP4/Singh et al. - 2022 - Know your audience specializing grounded language.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/SEIJMM73/2206.html:text/html}
}

@misc{lee_multi-game_2022,
  title     = {Multi-{Game} {Decision} {Transformers}},
  url       = {http://arxiv.org/abs/2205.15241},
  doi       = {10.48550/arXiv.2205.15241},
  abstract  = {A longstanding goal of the field of AI is a strategy for compiling diverse experience into a highly capable, generalist agent. In the subfields of vision and language, this was largely achieved by scaling up transformer-based models and training them on large, diverse datasets. Motivated by this progress, we investigate whether the same strategy can be used to produce generalist reinforcement learning agents. Specifically, we show that a single transformer-based model - with a single set of weights - trained purely offline can play a suite of up to 46 Atari games simultaneously at close-to-human performance. When trained and evaluated appropriately, we find that the same trends observed in language and vision hold, including scaling of performance with model size and rapid adaptation to new games via fine-tuning. We compare several approaches in this multi-game setting, such as online and offline RL methods and behavioral cloning, and find that our Multi-Game Decision Transformer models offer the best scalability and performance. We release the pre-trained models and code to encourage further research in this direction. Additional information, videos and code can be seen at: sites.google.com/view/multi-game-transformers},
  urldate   = {2022-08-09},
  publisher = {arXiv},
  author    = {Lee, Kuang-Huei and Nachum, Ofir and Yang, Mengjiao and Lee, Lisa and Freeman, Daniel and Xu, Winnie and Guadarrama, Sergio and Fischer, Ian and Jang, Eric and Michalewski, Henryk and Mordatch, Igor},
  month     = may,
  year      = {2022},
  note      = {arXiv:2205.15241 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  file      = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/P35J7JIV/Lee et al. - 2022 - Multi-Game Decision Transformers.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/QADEZ4TE/2205.html:text/html}
}

@misc{huang_inner_2022,
  title      = {Inner {Monologue}: {Embodied} {Reasoning} through {Planning} with {Language} {Models}},
  shorttitle = {Inner {Monologue}},
  url        = {http://arxiv.org/abs/2207.05608},
  doi        = {10.48550/arXiv.2207.05608},
  abstract   = {Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.},
  urldate    = {2022-08-09},
  publisher  = {arXiv},
  author     = {Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and Sermanet, Pierre and Brown, Noah and Jackson, Tomas and Luu, Linda and Levine, Sergey and Hausman, Karol and Ichter, Brian},
  month      = jul,
  year       = {2022},
  note       = {arXiv:2207.05608 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics},
  annote     = {Comment: Project website: https://innermonologue.github.io},
  file       = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/PXMU2AI5/Huang et al. - 2022 - Inner Monologue Embodied Reasoning through Planni.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/JA4LMIIA/2207.html:text/html}
}

@misc{ahn_as_2022,
  title      = {Do {As} {I} {Can}, {Not} {As} {I} {Say}: {Grounding} {Language} in {Robotic} {Affordances}},
  shorttitle = {Do {As} {I} {Can}, {Not} {As} {I} {Say}},
  url        = {http://arxiv.org/abs/2204.01691},
  doi        = {10.48550/arXiv.2204.01691},
  abstract   = {Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's "hands and eyes," while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at https://say-can.github.io/},
  urldate    = {2022-08-09},
  publisher  = {arXiv},
  author     = {Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Ho, Daniel and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jang, Eric and Ruano, Rosario Jauregui and Jeffrey, Kyle and Jesmonth, Sally and Joshi, Nikhil J. and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Luu, Linda and Parada, Carolina and Pastor, Peter and Quiambao, Jornell and Rao, Kanishka and Rettinghouse, Jarek and Reyes, Diego and Sermanet, Pierre and Sievers, Nicolas and Tan, Clayton and Toshev, Alexander and Vanhoucke, Vincent and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yan, Mengyuan},
  month      = apr,
  year       = {2022},
  note       = {arXiv:2204.01691 [cs]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics},
  annote     = {Comment: See website at https://say-can.github.io/},
  file       = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/YRRB3S6V/Ahn et al. - 2022 - Do As I Can, Not As I Say Grounding Language in R.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/93VGDJ22/2204.html:text/html}
}

@misc{xu_prompting_2022,
  title     = {Prompting {Decision} {Transformer} for {Few}-{Shot} {Policy} {Generalization}},
  url       = {http://arxiv.org/abs/2206.13499},
  doi       = {10.48550/arXiv.2206.13499},
  abstract  = {Humans can leverage prior experience and learn novel tasks from a handful of demonstrations. In contrast to offline meta-reinforcement learning, which aims to achieve quick adaptation through better algorithm design, we investigate the effect of architecture inductive bias on the few-shot learning capability. We propose a Prompt-based Decision Transformer (Prompt-DT), which leverages the sequential modeling ability of the Transformer architecture and the prompt framework to achieve few-shot adaptation in offline RL. We design the trajectory prompt, which contains segments of the few-shot demonstrations, and encodes task-specific information to guide policy generation. Our experiments in five MuJoCo control benchmarks show that Prompt-DT is a strong few-shot learner without any extra finetuning on unseen target tasks. Prompt-DT outperforms its variants and strong meta offline RL baselines by a large margin with a trajectory prompt containing only a few timesteps. Prompt-DT is also robust to prompt length changes and can generalize to out-of-distribution (OOD) environments.},
  urldate   = {2022-08-09},
  publisher = {arXiv},
  author    = {Xu, Mengdi and Shen, Yikang and Zhang, Shun and Lu, Yuchen and Zhao, Ding and Tenenbaum, Joshua B. and Gan, Chuang},
  month     = jun,
  year      = {2022},
  note      = {arXiv:2206.13499 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
  annote    = {Comment: ICML 2022. Project page: https://mxu34.github.io/PromptDT/},
  file      = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/3QBJHB8W/Xu et al. - 2022 - Prompting Decision Transformer for Few-Shot Policy.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/XA4E4E5R/2206.html:text/html}
}

@misc{noauthor_gql_nodate,
  title    = {gql},
  url      = {https://www.overleaf.com/project/625b2f3cc2e4833631da46c6},
  abstract = {An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
  language = {en},
  urldate  = {2022-08-02},
  file     = {Snapshot:/Users/ethan/Zotero/storage/IAI5M8QH/625b2f3cc2e4833631da46c6.html:text/html}
}

@misc{baumgartner_pushshift_2020,
  title     = {The {Pushshift} {Reddit} {Dataset}},
  url       = {http://arxiv.org/abs/2001.08435},
  doi       = {10.48550/arXiv.2001.08435},
  abstract  = {Social media data has become crucial to the advancement of scientific understanding. However, even though it has become ubiquitous, just collecting large-scale social media data involves a high degree of engineering skill set and computational resources. In fact, research is often times gated by data engineering problems that must be overcome before analysis can proceed. This has resulted recognition of datasets as meaningful research contributions in and of themselves. Reddit, the so called "front page of the Internet," in particular has been the subject of numerous scientific studies. Although Reddit is relatively open to data acquisition compared to social media platforms like Facebook and Twitter, the technical barriers to acquisition still remain. Thus, Reddit's millions of subreddits, hundreds of millions of users, and hundreds of billions of comments are at the same time relatively accessible, but time consuming to collect and analyze systematically. In this paper, we present the Pushshift Reddit dataset. Pushshift is a social media data collection, analysis, and archiving platform that since 2015 has collected Reddit data and made it available to researchers. Pushshift's Reddit dataset is updated in real-time, and includes historical data back to Reddit's inception. In addition to monthly dumps, Pushshift provides computational tools to aid in searching, aggregating, and performing exploratory analysis on the entirety of the dataset. The Pushshift Reddit dataset makes it possible for social media researchers to reduce time spent in the data collection, cleaning, and storage phases of their projects.},
  urldate   = {2022-08-02},
  publisher = {arXiv},
  author    = {Baumgartner, Jason and Zannettou, Savvas and Keegan, Brian and Squire, Megan and Blackburn, Jeremy},
  month     = jan,
  year      = {2020},
  note      = {Number: arXiv:2001.08435
               arXiv:2001.08435 [cs]},
  keywords  = {Computer Science - Computers and Society, Computer Science - Social and Information Networks},
  file      = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/CVDLUL54/Baumgartner et al. - 2020 - The Pushshift Reddit Dataset.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/IQDCZQ6L/2001.html:text/html}
}

@misc{zhang_opt_2022,
  title  = {{OPT}: {Open} {Pre}-trained {Transformer} {Language} {Models}},
  author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  year   = {2022},
  note   = {\_eprint: 2205.01068}
}

@misc{noauthor_openai_2021,
  title    = {{OpenAI} {API}},
  url      = {https://openai.com/api/},
  abstract = {OpenAI is an AI research and deployment company. Our mission is to ensure that artificial general intelligence benefits all of humanity.},
  language = {en},
  urldate  = {2022-05-19},
  month    = nov,
  year     = {2021},
  note     = {Publication Title: OpenAI},
  file     = {Snapshot:/Users/ethan/Zotero/storage/2C4PZG6I/api.html:text/html}
}

@inproceedings{wolf_transformers_2020,
  address    = {Online},
  title      = {Transformers: {State}-of-the-{Art} {Natural} {Language} {Processing}},
  shorttitle = {Transformers},
  url        = {https://aclanthology.org/2020.emnlp-demos.6},
  doi        = {10.18653/v1/2020.emnlp-demos.6},
  abstract   = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
  urldate    = {2022-01-22},
  booktitle  = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
  publisher  = {Association for Computational Linguistics},
  author     = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
  month      = oct,
  year       = {2020},
  pages      = {38--45}
}

@techreport{oh_self-imitation_2018,
  title       = {Self-{Imitation} {Learning}},
  url         = {http://arxiv.org/abs/1806.05635},
  abstract    = {This paper proposes Self-Imitation Learning (SIL), a simple off-policy actor-critic algorithm that learns to reproduce the agent's past good decisions. This algorithm is designed to verify our hypothesis that exploiting past good experiences can indirectly drive deep exploration. Our empirical results show that SIL significantly improves advantage actor-critic (A2C) on several hard exploration Atari games and is competitive to the state-of-the-art count-based exploration methods. We also show that SIL improves proximal policy optimization (PPO) on MuJoCo tasks.},
  number      = {arXiv:1806.05635},
  urldate     = {2022-05-19},
  institution = {arXiv},
  author      = {Oh, Junhyuk and Guo, Yijie and Singh, Satinder and Lee, Honglak},
  month       = jun,
  year        = {2018},
  keywords    = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
  annote      = {arXiv:1806.05635 [cs, stat] type: article},
  file        = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/9F2VLA2X/Oh et al. - 2018 - Self-Imitation Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/44QP5LTV/1806.html:text/html}
}

@article{min_rethinking_2022,
  title      = {Rethinking the {Role} of {Demonstrations}: {What} {Makes} {In}-{Context} {Learning} {Work}?},
  shorttitle = {Rethinking the {Role} of {Demonstrations}},
  url        = {http://arxiv.org/abs/2202.12837},
  abstract   = {Large language models (LMs) are able to in-context learn – perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required – randomly replacing labels in the demonstrations barely hurts performance, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.},
  urldate    = {2022-05-16},
  journal    = {arXiv:2202.12837 [cs]},
  author     = {Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  month      = feb,
  year       = {2022},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
  annote     = {arXiv: 2202.12837},
  annote     = {Comment: 14 pages; 11 figures; Code available at https://github.com/Alrope123/rethinking-demonstrations}
}

@article{chen_decision_2021,
  title      = {Decision {Transformer}: {Reinforcement} {Learning} via {Sequence} {Modeling}},
  shorttitle = {Decision {Transformer}},
  url        = {http://arxiv.org/abs/2106.01345},
  abstract   = {We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.},
  urldate    = {2022-05-18},
  journal    = {arXiv:2106.01345 [cs]},
  author     = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  month      = jun,
  year       = {2021},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  annote     = {arXiv: 2106.01345},
  annote     = {Comment: First two authors contributed equally. Last two authors advised equally}
}

@misc{wang_gpt-j-6b_2021,
  title  = {{GPT}-{J}-{6B}: {A} 6 {Billion} {Parameter} {Autoregressive} {Language} {Model}},
  url    = {https://github.com/kingoflolz/mesh-transformer-jax},
  author = {Wang, Ben and Komatsuzaki, Aran},
  month  = may,
  year   = {2021}
}

@inproceedings{ammanabrolu_learning_2021,
  title     = {Learning {Knowledge} {Graph}-based {World} {Models} of {Textual} {Environments}},
  volume    = {34},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/1e747ddbea997a1b933aaf58a7953c3c-Abstract.html},
  abstract  = {World models improve a learning agent's ability to efficiently operate in interactive and situated environments. This work focuses on the task of building world models of text-based game environments. Text-based games, or interactive narratives, are reinforcement learning environments in which agents perceive and interact with the world using textual natural language. These environments contain long, multi-step puzzles or quests woven through a world that is filled with hundreds of characters, locations, and objects. Our world model learns to simultaneously: (1) predict changes in the world caused by an agent's actions when representing the world as a knowledge graph; and (2) generate the set of contextually relevant natural language actions required to operate in the world. We frame this task as a Set of Sequences generation problem by exploiting the inherent structure of knowledge graphs and actions and introduce both a transformer-based multi-task architecture and a loss function to train it. A zero-shot ablation study on never-before-seen textual worlds shows that our methodology significantly outperforms existing textual world modeling techniques as well as the importance of each of our contributions.},
  urldate   = {2022-05-16},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Ammanabrolu, Prithviraj and Riedl, Mark},
  year      = {2021},
  pages     = {3720--3731}
}

@article{janner_offline_2021-2,
  title    = {Offline {Reinforcement} {Learning} as {One} {Big} {Sequence} {Modeling} {Problem}},
  url      = {http://arxiv.org/abs/2106.02039},
  abstract = {Reinforcement learning (RL) is typically concerned with estimating stationary policies or single-step models, leveraging the Markov property to factorize problems in time. However, we can also view RL as a generic sequence modeling problem, with the goal being to produce a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide effective solutions to the RL problem. To this end, we explore how RL can be tackled with the tools of sequence modeling, using a Transformer architecture to model distributions over trajectories and repurposing beam search as a planning algorithm. Framing RL as sequence modeling problem simplifies a range of design decisions, allowing us to dispense with many of the components common in offline RL algorithms. We demonstrate the flexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL. Further, we show that this approach can be combined with existing model-free algorithms to yield a state-of-the-art planner in sparse-reward, long-horizon tasks.},
  urldate  = {2022-05-18},
  journal  = {arXiv:2106.02039 [cs]},
  author   = {Janner, Michael and Li, Qiyang and Levine, Sergey},
  month    = nov,
  year     = {2021},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  annote   = {arXiv: 2106.02039},
  annote   = {Comment: NeurIPS 2021 (spotlight). Project page and code at: https://trajectory-transformer.github.io/}
}

@article{gao_pile_2020,
  title      = {The {Pile}: {An} {800GB} {Dataset} of {Diverse} {Text} for {Language} {Modeling}},
  shorttitle = {The {Pile}},
  url        = {http://arxiv.org/abs/2101.00027},
  abstract   = {Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present {\textbackslash}textbackslashtextit\{the Pile\}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets – both existing and newly constructed – many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.},
  urldate    = {2022-05-18},
  journal    = {arXiv:2101.00027 [cs]},
  author     = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  month      = dec,
  year       = {2020},
  keywords   = {Computer Science - Computation and Language},
  annote     = {arXiv: 2101.00027}
}

@article{fried_incoder_2022,
  title      = {{InCoder}: {A} {Generative} {Model} for {Code} {Infilling} and {Synthesis}},
  shorttitle = {{InCoder}},
  url        = {http://arxiv.org/abs/2204.05999},
  abstract   = {Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first generative model that is able to directly perform zero-shot code infilling, which we evaluate on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. The InCoder models and code are publicly released. https://sites.google.com/view/incoder-code-models},
  urldate    = {2022-05-15},
  journal    = {arXiv:2204.05999 [cs]},
  author     = {Fried, Daniel and Aghajanyan, Armen and Lin, Jessy and Wang, Sida and Wallace, Eric and Shi, Freda and Zhong, Ruiqi and Yih, Wen-tau and Zettlemoyer, Luke and Lewis, Mike},
  month      = apr,
  year       = {2022},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Software Engineering},
  annote     = {arXiv: 2204.05999},
  annote     = {Comment: 25 pages, 13 figures. v2: added NeoX-20B results \& StackOverflow corpus info}
}

@techreport{gao_pile_2020-1,
  title       = {The {Pile}: {An} {800GB} {Dataset} of {Diverse} {Text} for {Language} {Modeling}},
  shorttitle  = {The {Pile}},
  url         = {http://arxiv.org/abs/2101.00027},
  abstract    = {Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present {\textbackslash}textbackslashtextit\{the Pile\}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets – both existing and newly constructed – many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.},
  number      = {arXiv:2101.00027},
  urldate     = {2022-05-15},
  institution = {arXiv},
  author      = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  month       = dec,
  year        = {2020},
  doi         = {10.48550/arXiv.2101.00027},
  keywords    = {Computer Science - Computation and Language},
  annote      = {arXiv:2101.00027 [cs] type: article},
  file        = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/V3UJ4NPI/Gao et al. - 2020 - The Pile An 800GB Dataset of Diverse Text for Lan.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/9G86MCLG/2101.html:text/html}
}

@misc{noauthor_eleutheraigpt-j-6b_nodate,
  title    = {{EleutherAI}/gpt-j-{6B} · {Hugging} {Face}},
  url      = {https://huggingface.co/EleutherAI/gpt-j-6B},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  urldate  = {2022-05-15},
  file     = {Snapshot:/Users/ethan/Zotero/storage/Z5MQDH4C/gpt-j-6B.html:text/html}
}

@techreport{raffel_exploring_2020,
  title       = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
  url         = {http://arxiv.org/abs/1910.10683},
  abstract    = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  number      = {arXiv:1910.10683},
  urldate     = {2022-05-15},
  institution = {arXiv},
  author      = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  month       = jul,
  year        = {2020},
  keywords    = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
  annote      = {arXiv:1910.10683 [cs, stat] type: article},
  annote      = {Comment: Final version as published in JMLR},
  file        = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/3PKQSGQE/Raffel et al. - 2020 - Exploring the Limits of Transfer Learning with a U.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/4LQVVYZP/1910.html:text/html}
}

@article{rael_exploring_nodate,
  title    = {Exploring the {Limits} of {Transfer} {Learning} with a {Uniﬁed} {Text}-to-{Text} {Transformer}},
  language = {en},
  author   = {Raﬀel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  pages    = {67},
  file     = {Raﬀel et al. - Exploring the Limits of Transfer Learning with a U.pdf:/Users/ethan/Zotero/storage/WF329DNB/Raﬀel et al. - Exploring the Limits of Transfer Learning with a U.pdf:application/pdf}
}

@article{sun_conditioned_2020,
  title      = {Conditioned {Natural} {Language} {Generation} using only {Unconditioned} {Language} {Model}: {An} {Exploration}},
  shorttitle = {Conditioned {Natural} {Language} {Generation} using only {Unconditioned} {Language} {Model}},
  url        = {http://arxiv.org/abs/2011.07347},
  abstract   = {Transformer-based language models have shown to be very powerful for natural language generation (NLG). However, text generation conditioned on some user inputs, such as topics or attributes, is non-trivial. Past approach relies on either modifying the original LM architecture, re-training the LM on corpora with attribute labels, or having separately trained `guidance models' to guide text generation in decoding. We argued that the above approaches are not necessary, and the original unconditioned LM is sufficient for conditioned NLG. We evaluated our approaches by the samples' fluency and diversity with automated and human evaluation.},
  urldate    = {2022-05-15},
  journal    = {arXiv:2011.07347 [cs]},
  author     = {Sun, Fan-Keng and Lai, Cheng-I.},
  month      = nov,
  year       = {2020},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  annote     = {arXiv: 2011.07347}
}
@article{radford_language_nodate,
  title    = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  language = {en},
  author   = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  pages    = {24},
  file     = {Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:/Users/ethan/Zotero/storage/6DQWIJRH/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:application/pdf}
}

@techreport{chen_evaluating_2021,
  title       = {Evaluating {Large} {Language} {Models} {Trained} on {Code}},
  url         = {http://arxiv.org/abs/2107.03374},
  abstract    = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
  number      = {arXiv:2107.03374},
  urldate     = {2022-05-15},
  institution = {arXiv},
  author      = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  month       = jul,
  year        = {2021},
  keywords    = {Computer Science - Machine Learning},
  annote      = {arXiv:2107.03374 [cs] type: article},
  annote      = {Comment: corrected typos, added references, added authors, added acknowledgements},
  file        = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/3IX7KDC6/Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/KDJCHF8Q/2107.html:text/html}
}

@article{wei_chain_2022,
  title    = {Chain of {Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
  url      = {http://arxiv.org/abs/2201.11903},
  abstract = {Although scaling up language model size has reliably improved performance on a range of NLP tasks, even the largest models currently struggle with certain reasoning tasks such as math word problems, symbolic manipulation, and commonsense reasoning. This paper explores the ability of language models to generate a coherent chain of thought – a series of short sentences that mimic the reasoning process a person might have when responding to a question. Experiments show that inducing a chain of thought via prompting can enable sufficiently large language models to better perform reasoning tasks that otherwise have flat scaling curves. When combined with the 540B parameter PaLM model, chain of thought prompting achieves new state of the art of 58.1{\textbackslash}textbackslash\% on the GSM8K benchmark of math word problems.},
  urldate  = {2022-05-15},
  journal  = {arXiv:2201.11903 [cs]},
  author   = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  month    = apr,
  year     = {2022},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
  annote   = {arXiv: 2201.11903}
}

@article{schick_its_2021,
  title      = {It's {Not} {Just} {Size} {That} {Matters}: {Small} {Language} {Models} {Are} {Also} {Few}-{Shot} {Learners}},
  shorttitle = {It's {Not} {Just} {Size} {That} {Matters}},
  url        = {http://arxiv.org/abs/2009.07118},
  abstract   = {When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much "greener" in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.},
  urldate    = {2022-05-15},
  journal    = {arXiv:2009.07118 [cs]},
  author     = {Schick, Timo and Schütze, Hinrich},
  month      = apr,
  year       = {2021},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
  annote     = {arXiv: 2009.07118},
  annote     = {Comment: Accepted at NAACL2021}
}

@article{schick_exploiting_2021,
  title    = {Exploiting {Cloze} {Questions} for {Few} {Shot} {Text} {Classification} and {Natural} {Language} {Inference}},
  url      = {http://arxiv.org/abs/2001.07676},
  abstract = {Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with "task descriptions" in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.},
  urldate  = {2022-05-15},
  journal  = {arXiv:2001.07676 [cs]},
  author   = {Schick, Timo and Schütze, Hinrich},
  month    = jan,
  year     = {2021},
  keywords = {Computer Science - Computation and Language},
  annote   = {arXiv: 2001.07676},
  annote   = {Comment: Accepted at EACL2021}
}

@article{chowdhury_novelty_2022,
  title    = {Novelty {Controlled} {Paraphrase} {Generation} with {Retrieval} {Augmented} {Conditional} {Prompt} {Tuning}},
  url      = {http://arxiv.org/abs/2202.00535},
  abstract = {Paraphrase generation is a fundamental and long-standing task in natural language processing. In this paper, we concentrate on two contributions to the task: (1) we propose Retrieval Augmented Prompt Tuning (RAPT) as a parameter-efficient method to adapt large pre-trained language models for paraphrase generation; (2) we propose Novelty Conditioned RAPT (NC-RAPT) as a simple model-agnostic method of using specialized prompt tokens for controlled paraphrase generation with varying levels of lexical novelty. By conducting extensive experiments on four datasets, we demonstrate the effectiveness of the proposed approaches for retaining the semantic content of the original text while inducing lexical novelty in the generation.},
  urldate  = {2022-05-15},
  journal  = {arXiv:2202.00535 [cs]},
  author   = {Chowdhury, Jishnu Ray and Zhuang, Yong and Wang, Shuyi},
  month    = mar,
  year     = {2022},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
  annote   = {arXiv: 2202.00535},
  annote   = {Comment: Accepted by AAAI 2022 (Oral)}
}

@article{jiang_how_2020,
  title    = {How {Can} {We} {Know} {What} {Language} {Models} {Know}?},
  url      = {http://arxiv.org/abs/1911.12543},
  abstract = {Recent work has presented intriguing results examining the knowledge contained in language models (LM) by having the LM fill in the blanks of prompts such as "Obama is a \_ by profession". These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as "Obama worked as a \_" may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1\% to 39.6\%, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.},
  urldate  = {2022-05-15},
  journal  = {arXiv:1911.12543 [cs]},
  author   = {Jiang, Zhengbao and Xu, Frank F. and Araki, Jun and Neubig, Graham},
  month    = may,
  year     = {2020},
  keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  annote   = {arXiv: 1911.12543},
  annote   = {Comment: TACL 2020}
}

@article{peng_reducing_2020,
  title    = {Reducing {Non}-{Normative} {Text} {Generation} from {Language} {Models}},
  url      = {http://arxiv.org/abs/2001.08764},
  abstract = {Large-scale, transformer-based language models such as GPT-2 are pretrained on diverse corpora scraped from the internet. Consequently, they are prone to generating non-normative text (i.e. in violation of social norms). We introduce a technique for fine-tuning GPT-2, using a policy gradient reinforcement learning technique and a normative text classifier to produce reward and punishment values. We evaluate our technique on five data sets using automated and human participant experiments. The normative text classifier is 81-90\% accurate when compared to gold-standard human judgments of normative and non-normative generated text. Our normative fine-tuning technique is able to reduce non-normative text by 27-61\%, depending on the data set.},
  urldate  = {2022-05-15},
  journal  = {arXiv:2001.08764 [cs]},
  author   = {Peng, Xiangyu and Li, Siyan and Frazier, Spencer and Riedl, Mark},
  month    = oct,
  year     = {2020},
  keywords = {Computer Science - Computation and Language},
  annote   = {arXiv: 2001.08764}
}

@article{kang_neural_2020,
  title      = {Neural {Mask} {Generator}: {Learning} to {Generate} {Adaptive} {Word} {Maskings} for {Language} {Model} {Adaptation}},
  shorttitle = {Neural {Mask} {Generator}},
  url        = {http://arxiv.org/abs/2010.02705},
  abstract   = {We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the language model to a particular target task (e.g. question answering). Specifically, we present a novel reinforcement learning-based framework which learns the masking policy, such that using the generated masks for further pre-training of the target language model helps improve task performance on unseen texts. We use off-policy actor-critic with entropy regularization and experience replay for reinforcement learning, and propose a Transformer-based policy network that can consider the relative importance of words in a given text. We validate our Neural Mask Generator (NMG) on several question answering and text classification datasets using BERT and DistilBERT as the language models, on which it outperforms rule-based masking strategies, by automatically learning optimal adaptive maskings.},
  urldate    = {2022-05-15},
  journal    = {arXiv:2010.02705 [cs]},
  author     = {Kang, Minki and Han, Moonsu and Hwang, Sung Ju},
  month      = oct,
  year       = {2020},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
  annote     = {arXiv: 2010.02705},
  annote     = {Comment: 19 pages, 9 figures, EMNLP 2020}
}

@article{lv_commonsense_2022,
  title     = {Commonsense {Knowledge}-{Aware} {Prompt} {Tuning} for {Few}-{Shot} {NOTA} {Relation} {Classification}},
  volume    = {12},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  issn      = {2076-3417},
  url       = {https://www.mdpi.com/2076-3417/12/4/2185},
  doi       = {10.3390/app12042185},
  abstract  = {Compared with the traditional few-shot task, the few-shot none-of-the-above (NOTA) relation classification focuses on the realistic scenario of few-shot learning, in which a test instance might not belong to any of the target categories. This undoubtedly increases the task’s difficulty because given only a few support samples, this cannot represent the distribution of NOTA categories in space. The model needs to make full use of the syntactic information and word meaning information learned in the pre-training stage to distinguish the NOTA category and the support sample category in the embedding space. However, previous fine-tuning methods mainly focus on optimizing the extra classifiers (on top of pre-trained language models (PLMs)) and neglect the connection between pre-training objectives and downstream tasks. In this paper, we propose the commonsense knowledge-aware prompt tuning (CKPT) method for a few-shot NOTA relation classification task. First, a simple and effective prompt-learning method is developed by constructing relation-oriented templates, which can further stimulate the rich knowledge distributed in PLMs to better serve downstream tasks. Second, external knowledge is incorporated into the model by a label-extension operation, which forms knowledgeable prompt tuning to improve and stabilize prompt tuning. Third, to distinguish the NOTA pairs and positive pairs in embedding space more accurately, a learned scoring strategy is proposed, which introduces a learned threshold classification function and improves the loss function by adding a new term focused on NOTA identification. Experiments on two widely used benchmarks (FewRel 2.0 and Few-shot TACRED) show that our method is a simple and effective framework, and a new state of the art is established in the few-shot classification field.},
  language  = {en},
  number    = {4},
  urldate   = {2022-05-15},
  journal   = {Applied Sciences},
  author    = {Lv, Bo and Jin, Li and Zhang, Yanan and Wang, Hao and Li, Xiaoyu and Guo, Zhi},
  month     = jan,
  year      = {2022},
  keywords  = {commonsense knowledge-aware prompt tuning, few-shot none-of-the-above relation classification, pre-trained language models, scoring strategy},
  pages     = {2185},
  annote    = {Number: 4 Publisher: Multidisciplinary Digital Publishing Institute}
}

@article{li_prefix-tuning_2021,
  title      = {Prefix-{Tuning}: {Optimizing} {Continuous} {Prompts} for {Generation}},
  shorttitle = {Prefix-{Tuning}},
  url        = {https://arxiv.org/abs/2101.00190v1},
  doi        = {10.48550/arXiv.2101.00190},
  abstract   = {Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were "virtual tokens". We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1{\textbackslash}textbackslash\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.},
  language   = {en},
  urldate    = {2022-05-15},
  author     = {Li, Xiang Lisa and Liang, Percy},
  month      = jan,
  year       = {2021}
}

@article{mnih_human-level_2015,
  title     = {Human-level control through deep reinforcement learning},
  volume    = {518},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn      = {1476-4687},
  url       = {https://www.nature.com/articles/nature14236},
  doi       = {10.1038/nature14236},
  abstract  = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
  language  = {en},
  number    = {7540},
  urldate   = {2022-05-14},
  journal   = {Nature},
  author    = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  month     = feb,
  year      = {2015},
  keywords  = {Computer science},
  pages     = {529--533},
  annote    = {Number: 7540 Publisher: Nature Publishing Group},
  file      = {Full Text PDF:/Users/ethan/Zotero/storage/FLIH28J9/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf;Snapshot:/Users/ethan/Zotero/storage/HDYP8GCZ/nature14236.html:text/html}
}

@article{lester_power_2021,
  title    = {The {Power} of {Scale} for {Parameter}-{Efficient} {Prompt} {Tuning}},
  url      = {https://arxiv.org/abs/2104.08691v2},
  doi      = {10.48550/arXiv.2104.08691},
  abstract = {In this work, we explore "prompt tuning", a simple yet effective mechanism for learning "soft prompts" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's "few-shot" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method "closes the gap" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed "prefix tuning" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.},
  language = {en},
  urldate  = {2022-05-15},
  author   = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  month    = apr,
  year     = {2021}
}

@article{shin_autoprompt_2020,
  title      = {{AutoPrompt}: {Eliciting} {Knowledge} from {Language} {Models} with {Automatically} {Generated} {Prompts}},
  shorttitle = {{AutoPrompt}},
  url        = {https://arxiv.org/abs/2010.15980v1},
  doi        = {10.48550/arXiv.2010.15980},
  abstract   = {The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.},
  language   = {en},
  urldate    = {2022-05-15},
  author     = {Shin, Taylor and Razeghi, Yasaman and Logan IV, Robert L. and Wallace, Eric and Singh, Sameer},
  month      = oct,
  year       = {2020}
}

@article{hill_human_2020,
  title    = {Human {Instruction}-{Following} with {Deep} {Reinforcement} {Learning} via {Transfer}-{Learning} from {Text}},
  url      = {http://arxiv.org/abs/2005.09382},
  abstract = {Recent work has described neural-network-based agents that are trained with reinforcement learning (RL) to execute language-like commands in simulated worlds, as a step towards an intelligent agent or robot that can be instructed by human users. However, the optimisation of multi-goal motor policies via deep RL from scratch requires many episodes of experience. Consequently, instruction-following with deep RL typically involves language generated from templates (by an environment simulator), which does not reflect the varied or ambiguous expressions of real users. Here, we propose a conceptually simple method for training instruction-following agents with deep RL that are robust to natural human instructions. By applying our method with a state-of-the-art pre-trained text-based language model (BERT), on tasks requiring agents to identify and position everyday objects relative to other objects in a naturalistic 3D simulated room, we demonstrate substantially-above-chance zero-shot transfer from synthetic template commands to natural instructions given by humans. Our approach is a general recipe for training any deep RL-based system to interface with human users, and bridges the gap between two research directions of notable recent success: agent-centric motor behavior and text-based representation learning.},
  urldate  = {2022-05-15},
  journal  = {arXiv:2005.09382 [cs]},
  author   = {Hill, Felix and Mokra, Sona and Wong, Nathaniel and Harley, Tim},
  month    = may,
  year     = {2020},
  keywords = {Computer Science - Computation and Language},
  annote   = {arXiv: 2005.09382}
}

@article{lu_pretrained_2021,
  title    = {Pretrained {Transformers} as {Universal} {Computation} {Engines}},
  url      = {http://arxiv.org/abs/2103.05247},
  abstract = {We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning – in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language improves performance and compute efficiency on non-language downstream tasks. In particular, we find that such pretraining enables FPT to generalize in zero-shot to these modalities, matching the performance of a transformer fully trained on these tasks.},
  urldate  = {2021-04-23},
  journal  = {arXiv:2103.05247 [cs]},
  author   = {Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor},
  month    = mar,
  year     = {2021},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  annote   = {arXiv: 2103.05247}
}

@techreport{wei_chain_2022-1,
  title       = {Chain of {Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
  url         = {http://arxiv.org/abs/2201.11903},
  abstract    = {Although scaling up language model size has reliably improved performance on a range of NLP tasks, even the largest models currently struggle with certain reasoning tasks such as math word problems, symbolic manipulation, and commonsense reasoning. This paper explores the ability of language models to generate a coherent chain of thought – a series of short sentences that mimic the reasoning process a person might have when responding to a question. Experiments show that inducing a chain of thought via prompting can enable sufficiently large language models to better perform reasoning tasks that otherwise have flat scaling curves. When combined with the 540B parameter PaLM model, chain of thought prompting achieves new state of the art of 58.1{\textbackslash}textbackslash\% on the GSM8K benchmark of math word problems.},
  number      = {arXiv:2201.11903},
  urldate     = {2022-05-14},
  institution = {arXiv},
  author      = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  month       = apr,
  year        = {2022},
  doi         = {10.48550/arXiv.2201.11903},
  keywords    = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
  annote      = {arXiv:2201.11903 [cs] type: article},
  file        = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/DBWVEW3F/Wei et al. - 2022 - Chain of Thought Prompting Elicits Reasoning in La.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/3PMQ9RG2/2201.html:text/html}
}

@article{mnih_human-level_2015-1,
  title     = {Human-level control through deep reinforcement learning},
  volume    = {518},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn      = {1476-4687},
  url       = {https://www.nature.com/articles/nature14236},
  doi       = {10.1038/nature14236},
  abstract  = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
  language  = {en},
  number    = {7540},
  urldate   = {2022-05-14},
  journal   = {Nature},
  author    = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  month     = feb,
  year      = {2015},
  keywords  = {Computer science},
  pages     = {529--533},
  annote    = {Number: 7540 Publisher: Nature Publishing Group},
  file      = {Full Text PDF:/Users/ethan/Zotero/storage/B3YU6AYT/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf;Snapshot:/Users/ethan/Zotero/storage/4NS3VX37/nature14236.html:text/html}
}

@inproceedings{brown_language_2020,
  title     = {Language {Models} are {Few}-{Shot} {Learners}},
  volume    = {33},
  url       = {https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  abstract  = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
  urldate   = {2022-05-14},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year      = {2020},
  pages     = {1877--1901},
  file      = {Full Text PDF:/Users/ethan/Zotero/storage/LMADAA5T/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf}
}

@article{brown_language_2020-1,
  title    = {Language {Models} are {Few}-{Shot} {Learners}},
  url      = {http://arxiv.org/abs/2005.14165},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  urldate  = {2021-03-28},
  journal  = {arXiv:2005.14165 [cs]},
  author   = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  month    = jul,
  year     = {2020},
  keywords = {Computer Science - Computation and Language},
  annote   = {arXiv: 2005.14165},
  annote   = {Comment: 40+32 pages},
  file     = {arXiv.org Snapshot:/Users/ethan/Zotero/storage/EZGADYUD/2005.html:text/html}
}

@article{tam_semantic_2022,
  title    = {Semantic {Exploration} from {Language} {Abstractions} and {Pretrained} {Representations}},
  url      = {http://arxiv.org/abs/2204.05080},
  abstract = {Continuous first-person 3D environments pose unique exploration challenges to reinforcement learning (RL) agents because of their high-dimensional state and action spaces. These challenges can be ameliorated by using semantically meaningful state abstractions to define novelty for exploration. We propose that learned representations shaped by natural language provide exactly this form of abstraction. In particular, we show that vision-language representations, when pretrained on image captioning datasets sampled from the internet, can drive meaningful, task-relevant exploration and improve performance on 3D simulated environments. We also characterize why and how language provides useful abstractions for exploration by comparing the impacts of using representations from a pretrained model, a language oracle, and several ablations. We demonstrate the benefits of our approach in two very different task domains – one that stresses the identification and manipulation of everyday objects, and one that requires navigational exploration in an expansive world – as well as two popular deep RL algorithms: Impala and R2D2. Our results suggest that using language-shaped representations could improve exploration for various algorithms and agents in challenging environments.},
  urldate  = {2022-05-14},
  journal  = {arXiv:2204.05080 [cs]},
  author   = {Tam, Allison C. and Rabinowitz, Neil C. and Lampinen, Andrew K. and Roy, Nicholas A. and Chan, Stephanie C. Y. and Strouse, D. J. and Wang, Jane X. and Banino, Andrea and Hill, Felix},
  month    = apr,
  year     = {2022},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  annote   = {arXiv: 2204.05080},
  file     = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/7SNF2KBV/Tam et al. - 2022 - Semantic Exploration from Language Abstractions an.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/2RNVMSJ8/2204.html:text/html}
}

@article{huang_language_2022,
  title      = {Language {Models} as {Zero}-{Shot} {Planners}: {Extracting} {Actionable} {Knowledge} for {Embodied} {Agents}},
  shorttitle = {Language {Models} as {Zero}-{Shot} {Planners}},
  url        = {http://arxiv.org/abs/2201.07207},
  abstract   = {Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. "make breakfast"), to a chosen set of actionable steps (e.g. "open fridge"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at https://huangwl18.github.io/language-planner},
  urldate    = {2022-05-14},
  journal    = {arXiv:2201.07207 [cs]},
  author     = {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  month      = mar,
  year       = {2022},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics},
  annote     = {arXiv: 2201.07207},
  annote     = {Comment: Project website at https://huangwl18.github.io/language-planner},
  file       = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/JFEN5S9F/Huang et al. - 2022 - Language Models as Zero-Shot Planners Extracting .pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/PNVFUACB/2201.html:text/html}
}

@article{garg_lisa_2022,
  title      = {{LISA}: {Learning} {Interpretable} {Skill} {Abstractions} from {Language}},
  shorttitle = {{LISA}},
  url        = {http://arxiv.org/abs/2203.00054},
  abstract   = {Learning policies that effectually utilize language instructions in complex, multi-task environments is an important problem in imitation learning. While it is possible to condition on the entire language instruction directly, such an approach could suffer from generalization issues. To encode complex instructions into skills that can generalize to unseen instructions, we propose Learning Interpretable Skill Abstractions (LISA), a hierarchical imitation learning framework that can learn diverse, interpretable skills from language-conditioned demonstrations. LISA uses vector quantization to learn discrete skill codes that are highly correlated with language instructions and the behavior of the learned policy. In navigation and robotic manipulation environments, LISA is able to outperform a strong non-hierarchical baseline in the low data regime and compose learned skills to solve tasks containing unseen long-range instructions. Our method demonstrates a more natural way to condition on language in sequential decision-making problems and achieve interpretable and controllable behavior with the learned skills.},
  urldate    = {2022-05-14},
  journal    = {arXiv:2203.00054 [cs]},
  author     = {Garg, Divyansh and Vaidyanath, Skanda and Kim, Kuno and Song, Jiaming and Ermon, Stefano},
  month      = feb,
  year       = {2022},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics},
  annote     = {arXiv: 2203.00054},
  file       = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/MU74BD4P/Garg et al. - 2022 - LISA Learning Interpretable Skill Abstractions fr.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/CAZI33YE/2203.html:text/html}
}

@article{li_pre-trained_2022,
  title    = {Pre-{Trained} {Language} {Models} for {Interactive} {Decision}-{Making}},
  url      = {http://arxiv.org/abs/2202.01771},
  abstract = {Language model (LM) pre-training has proven useful for a wide variety of language processing tasks, but can such pre-training be leveraged for more general machine learning problems? We investigate the effectiveness of language modeling to scaffold learning and generalization in autonomous decision-making. We describe a framework for imitation learning in which goals and observations are represented as a sequence of embeddings, and translated into actions using a policy network initialized with a pre-trained transformer LM. We demonstrate that this framework enables effective combinatorial generalization across different environments, such as VirtualHome and BabyAI. In particular, for test tasks involving novel goals or novel scenes, initializing policies with language models improves task completion rates by 43.6\% in VirtualHome. We hypothesize and investigate three possible factors underlying the effectiveness of LM-based policy initialization. We find that sequential representations (vs. fixed-dimensional feature vectors) and the LM objective (not just the transformer architecture) are both important for generalization. Surprisingly, however, the format of the policy inputs encoding (e.g. as a natural language string vs. an arbitrary sequential encoding) has little influence. Together, these results suggest that language modeling induces representations that are useful for modeling not just language, but also goals and plans; these representations can aid learning and generalization even outside of language processing.},
  urldate  = {2022-05-14},
  journal  = {arXiv:2202.01771 [cs]},
  author   = {Li, Shuang and Puig, Xavier and Paxton, Chris and Du, Yilun and Wang, Clinton and Fan, Linxi and Chen, Tao and Huang, De-An and Akyürek, Ekin and Anandkumar, Anima and Andreas, Jacob and Mordatch, Igor and Torralba, Antonio and Zhu, Yuke},
  month    = feb,
  year     = {2022},
  keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  annote   = {arXiv: 2202.01771},
  file     = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/J5JGITZX/Li et al. - 2022 - Pre-Trained Language Models for Interactive Decisi.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/G7TTEL8X/2202.html:text/html}
}

@article{peng_inherently_2021,
  title    = {Inherently {Explainable} {Reinforcement} {Learning} in {Natural} {Language}},
  url      = {http://arxiv.org/abs/2112.08907},
  abstract = {We focus on the task of creating a reinforcement learning agent that is inherently explainable – with the ability to produce immediate local explanations by thinking out loud while performing a task and analyzing entire trajectories post-hoc to produce causal explanations. This Hierarchically Explainable Reinforcement Learning agent (HEX-RL), operates in Interactive Fictions, text-based game environments in which an agent perceives and acts upon the world using textual natural language. These games are usually structured as puzzles or quests with long-term dependencies in which an agent must complete a sequence of actions to succeed – providing ideal environments in which to test an agent's ability to explain its actions. Our agent is designed to treat explainability as a first-class citizen, using an extracted symbolic knowledge graph-based state representation coupled with a Hierarchical Graph Attention mechanism that points to the facts in the internal graph representation that most influenced the choice of actions. Experiments show that this agent provides significantly improved explanations over strong baselines, as rated by human participants generally unfamiliar with the environment, while also matching state-of-the-art task performance.},
  urldate  = {2022-05-14},
  journal  = {arXiv:2112.08907 [cs]},
  author   = {Peng, Xiangyu and Riedl, Mark O. and Ammanabrolu, Prithviraj},
  month    = dec,
  year     = {2021},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
  annote   = {arXiv: 2112.08907},
  file     = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/57WBV3AR/Peng et al. - 2021 - Inherently Explainable Reinforcement Learning in N.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/ABDIYGCT/2112.html:text/html}
}

@article{jiang_language_2019,
  title    = {Language as an {Abstraction} for {Hierarchical} {Deep} {Reinforcement} {Learning}},
  url      = {http://arxiv.org/abs/1906.07343},
  abstract = {Solving complex, temporally-extended tasks is a long-standing problem in reinforcement learning (RL). We hypothesize that one critical element of solving such problems is the notion of compositionality. With the ability to learn concepts and sub-skills that can be composed to solve longer tasks, i.e. hierarchical RL, we can acquire temporally-extended behaviors. However, acquiring effective yet general abstractions for hierarchical RL is remarkably challenging. In this paper, we propose to use language as the abstraction, as it provides unique compositional structure, enabling fast learning and combinatorial generalization, while retaining tremendous flexibility, making it suitable for a variety of problems. Our approach learns an instruction-following low-level policy and a high-level policy that can reuse abstractions across tasks, in essence, permitting agents to reason using structured language. To study compositional task learning, we introduce an open-source object interaction environment built using the MuJoCo physics engine and the CLEVR engine. We find that, using our approach, agents can learn to solve to diverse, temporally-extended tasks such as object sorting and multi-object rearrangement, including from raw pixel observations. Our analysis reveals that the compositional nature of language is critical for learning diverse sub-skills and systematically generalizing to new sub-skills in comparison to non-compositional abstractions that use the same supervision.},
  urldate  = {2022-05-14},
  journal  = {arXiv:1906.07343 [cs, stat]},
  author   = {Jiang, Yiding and Gu, Shixiang and Murphy, Kevin and Finn, Chelsea},
  month    = nov,
  year     = {2019},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
  annote   = {arXiv: 1906.07343},
  annote   = {Comment: Published in Neural Information Processing Systems (NeurIPS) 2019; Supplementary materials: https://sites.google.com/view/hal-demo},
  file     = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/IBKQAQDP/Jiang et al. - 2019 - Language as an Abstraction for Hierarchical Deep R.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/3N9BMRW7/1906.html:text/html}
}

@article{chowdhery_palm_2022,
  title      = {{PaLM}: {Scaling} {Language} {Modeling} with {Pathways}},
  shorttitle = {{PaLM}},
  url        = {http://arxiv.org/abs/2204.02311},
  abstract   = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
  urldate    = {2022-04-10},
  journal    = {arXiv:2204.02311 [cs]},
  author     = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  month      = apr,
  year       = {2022},
  keywords   = {Computer Science - Computation and Language},
  annote     = {arXiv: 2204.02311},
  file       = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/CG2RZ83A/Chowdhery et al. - 2022 - PaLM Scaling Language Modeling with Pathways.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/2PLFWWS5/2204.html:text/html}
}

@article{reed_generalist_2022-1,
  title    = {A {Generalist} {Agent}},
  url      = {http://arxiv.org/abs/2205.06175},
  abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
  urldate  = {2022-05-14},
  journal  = {arXiv:2205.06175 [cs]},
  author   = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and de Freitas, Nando},
  month    = may,
  year     = {2022},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics},
  annote   = {
              },
  annote   = {arXiv: 2205.06175}
}

@article{luketina_survey_2019,
  title    = {A {Survey} of {Reinforcement} {Learning} {Informed} by {Natural} {Language}},
  url      = {http://arxiv.org/abs/1906.03926},
  abstract = {To be successful in real-world tasks, Reinforcement Learning (RL) needs to exploit the compositional, relational, and hierarchical structure of the world, and learn to transfer it to the task at hand. Recent advances in representation learning for language make it possible to build models that acquire world knowledge from text corpora and integrate this knowledge into downstream decision making problems. We thus argue that the time is right to investigate a tight integration of natural language understanding into RL in particular. We survey the state of the field, including work on instruction following, text games, and learning from textual domain knowledge. Finally, we call for the development of new environments as well as further investigation into the potential uses of recent Natural Language Processing (NLP) techniques for such tasks.},
  urldate  = {2022-05-14},
  journal  = {arXiv:1906.03926 [cs, stat]},
  author   = {Luketina, Jelena and Nardelli, Nantas and Farquhar, Gregory and Foerster, Jakob and Andreas, Jacob and Grefenstette, Edward and Whiteson, Shimon and Rocktäschel, Tim},
  month    = jun,
  year     = {2019},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
  annote   = {arXiv: 1906.03926},
  annote   = {Comment: Published at IJCAI'19},
  file     = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/4N4G74VG/Luketina et al. - 2019 - A Survey of Reinforcement Learning Informed by Nat.pdf:application/pdf}
}

@article{yao_keep_2020,
  title      = {Keep {CALM} and {Explore}: {Language} {Models} for {Action} {Generation} in {Text}-based {Games}},
  shorttitle = {Keep {CALM} and {Explore}},
  url        = {http://arxiv.org/abs/2010.02903},
  abstract   = {Text-based games present a unique challenge for autonomous agents to operate in natural language and handle enormous action spaces. In this paper, we propose the Contextual Action Language Model (CALM) to generate a compact set of action candidates at each game state. Our key insight is to train language models on human gameplay, where people demonstrate linguistic priors and a general game sense for promising actions conditioned on game history. We combine CALM with a reinforcement learning agent which re-ranks the generated action candidates to maximize in-game rewards. We evaluate our approach using the Jericho benchmark, on games unseen by CALM during training. Our method obtains a 69\% relative improvement in average game score over the previous state-of-the-art model. Surprisingly, on half of these games, CALM is competitive with or better than other models that have access to ground truth admissible actions. Code and data are available at https://github.com/princeton-nlp/calm-textgame.},
  urldate    = {2022-05-14},
  journal    = {arXiv:2010.02903 [cs]},
  author     = {Yao, Shunyu and Rao, Rohan and Hausknecht, Matthew and Narasimhan, Karthik},
  month      = oct,
  year       = {2020},
  keywords   = {Computer Science - Computation and Language},
  annote     = {arXiv: 2010.02903},
  annote     = {Comment: EMNLP 2020}
}

@article{scheurer_training_2022,
  title    = {Training {Language} {Models} with {Natural} {Language} {Feedback}},
  url      = {http://arxiv.org/abs/2204.14146},
  abstract = {Pretrained language models often do not perform tasks in ways that are in line with our preferences, e.g., generating offensive text or factually incorrect summaries. Recent work approaches the above issue by learning from a simple form of human evaluation: comparisons between pairs of model-generated task outputs. Comparison feedback conveys limited information about human preferences per human evaluation. Here, we propose to learn from natural language feedback, which conveys more information per human evaluation. We learn from language feedback on model outputs using a three-step learning algorithm. First, we condition the language model on the initial output and feedback to generate many refinements. Second, we choose the refinement with the highest similarity to the feedback. Third, we finetune a language model to maximize the likelihood of the chosen refinement given the input. In synthetic experiments, we first evaluate whether language models accurately incorporate feedback to produce refinements, finding that only large language models (175B parameters) do so. Using only 100 samples of human-written feedback, our learning algorithm finetunes a GPT-3 model to roughly human-level summarization.},
  urldate  = {2022-05-14},
  journal  = {arXiv:2204.14146 [cs]},
  author   = {Scheurer, Jérémy and Campos, Jon Ander and Chan, Jun Shern and Chen, Angelica and Cho, Kyunghyun and Perez, Ethan},
  month    = may,
  year     = {2022},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
  annote   = {arXiv: 2204.14146},
  annote   = {Comment: The First Workshop on Learning with Natural Language Supervision at ACL 2022},
  file     = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/YA4FQI24/Scheurer et al. - 2022 - Training Language Models with Natural Language Fee.pdf:application/pdf}
}

@article{reid_can_2022,
  title    = {Can {Wikipedia} {Help} {Offline} {Reinforcement} {Learning}?},
  url      = {http://arxiv.org/abs/2201.12122},
  abstract = {Fine-tuning reinforcement learning (RL) models has been challenging because of a lack of large scale off-the-shelf datasets as well as high variance in transferability among different environments. Recent work has looked at tackling offline RL from the perspective of sequence modeling with improved results as result of the introduction of the Transformer architecture. However, when the model is trained from scratch, it suffers from slow convergence speeds. In this paper, we look to take advantage of this formulation of reinforcement learning as sequence modeling and investigate the transferability of pre-trained sequence models on other domains (vision, language) when finetuned on offline RL tasks (control, games). To this end, we also propose techniques to improve transfer between these domains. Results show consistent performance gains in terms of both convergence speed and reward on a variety of environments, accelerating training by 3-6x and achieving state-of-the-art performance in a variety of tasks using Wikipedia-pretrained and GPT2 language models. We hope that this work not only brings light to the potentials of leveraging generic sequence modeling techniques and pre-trained models for RL, but also inspires future work on sharing knowledge between generative modeling tasks of completely different domains.},
  urldate  = {2022-05-14},
  journal  = {arXiv:2201.12122 [cs]},
  author   = {Reid, Machel and Yamada, Yutaro and Gu, Shixiang Shane},
  month    = mar,
  year     = {2022},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
  annote   = {arXiv: 2201.12122}
}

@inproceedings{tarasov_prompts_2022,
  title    = {Prompts and {Pre}-{Trained} {Language} {Models} for {Offline} {Reinforcement} {Learning}},
  url      = {https://openreview.net/forum?id=Spf4TE6NkWq},
  abstract = {Prompt engineering can be successfully used for deep offline reinforcement learning in environments that are not naturally suited for the textual representation.},
  language = {en},
  urldate  = {2022-05-14},
  author   = {Tarasov, Denis and Kurenkov, Vladislav and Kolesnikov, Sergey},
  month    = mar,
  year     = {2022}
}

@article{madureira_overview_2020,
  title    = {An {Overview} of {Natural} {Language} {State} {Representation} for {Reinforcement} {Learning}},
  url      = {http://arxiv.org/abs/2007.09774},
  abstract = {A suitable state representation is a fundamental part of the learning process in Reinforcement Learning. In various tasks, the state can either be described by natural language or be natural language itself. This survey outlines the strategies used in the literature to build natural language state representations. We appeal for more linguistically interpretable and grounded representations, careful justification of design decisions and evaluation of the effectiveness of different approaches.},
  urldate  = {2022-05-14},
  journal  = {arXiv:2007.09774 [cs]},
  author   = {Madureira, Brielen and Schlangen, David},
  month    = jul,
  year     = {2020},
  keywords = {Computer Science - Computation and Language},
  annote   = {arXiv: 2007.09774},
  annote   = {Comment: Accepted to the ICML 2020 Workshop on Language in Reinforcement Learning (LaReL). 4 pages}
}

@article{singh_pre-trained_2021,
  title    = {Pre-trained {Language} {Models} as {Prior} {Knowledge} for {Playing} {Text}-based {Games}},
  url      = {http://arxiv.org/abs/2107.08408},
  abstract = {Recently, text world games have been proposed to enable artificial agents to understand and reason about real-world scenarios. These text-based games are challenging for artificial agents, as it requires an understanding of and interaction using natural language in a partially observable environment. Agents observe the environment via textual descriptions designed to be challenging enough for even human players. Past approaches have not paid enough attention to the language understanding capability of the proposed agents. Typically, these approaches train from scratch, an agent that learns both textual representations and the gameplay online during training using a temporal loss function. Given the sample-inefficiency of RL approaches, it is inefficient to learn rich enough textual representations to be able to understand and reason using the textual observation in such a complicated game environment setting. In this paper, we improve the semantic understanding of the agent by proposing a simple RL with LM framework where we use transformer-based language models with Deep RL models. We perform a detailed study of our framework to demonstrate how our model outperforms all existing agents on the popular game, Zork1, to achieve a score of 44.7, which is 1.6 higher than the state-of-the-art model. Overall, our proposed approach outperforms 4 games out of the 14 text-based games, while performing comparable to the state-of-the-art models on the remaining games.},
  urldate  = {2022-05-14},
  journal  = {arXiv:2107.08408 [cs]},
  author   = {Singh, Ishika and Singh, Gargi and Modi, Ashutosh},
  month    = dec,
  year     = {2021},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Robotics, Computer Science - Multiagent Systems},
  annote   = {arXiv: 2107.08408},
  annote   = {Comment: 40 Pages (8 Pages main content + 1 Page references + 31 Pages Appendix). Some new results added}
}

@article{majumdar_improving_2020,
  title    = {Improving {Vision}-and-{Language} {Navigation} with {Image}-{Text} {Pairs} from the {Web}},
  url      = {http://arxiv.org/abs/2004.14973},
  abstract = {Following a navigation instruction such as 'Walk down the stairs and stop at the brown sofa' requires embodied AI agents to ground scene elements referenced via language (e.g. 'stairs') to visual content in the environment (pixels corresponding to 'stairs'). We ask the following question – can we leverage abundant 'disembodied' web-scraped vision-and-language corpora (e.g. Conceptual Captions) to learn visual groundings (what do 'stairs' look like?) that improve performance on a relatively data-starved embodied perception task (Vision-and-Language Navigation)? Specifically, we develop VLN-BERT, a visiolinguistic transformer-based model for scoring the compatibility between an instruction ('...stop at the brown sofa') and a sequence of panoramic RGB images captured by the agent. We demonstrate that pretraining VLN-BERT on image-text pairs from the web before fine-tuning on embodied path-instruction data significantly improves performance on VLN – outperforming the prior state-of-the-art in the fully-observed setting by 4 absolute percentage points on success rate. Ablations of our pretraining curriculum show each stage to be impactful – with their combination resulting in further positive synergistic effects.},
  urldate  = {2022-05-14},
  journal  = {arXiv:2004.14973 [cs]},
  author   = {Majumdar, Arjun and Shrivastava, Ayush and Lee, Stefan and Anderson, Peter and Parikh, Devi and Batra, Dhruv},
  month    = may,
  year     = {2020},
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
  annote   = {arXiv: 2004.14973}
}

@misc{garg_what_2022,
  title      = {What {Can} {Transformers} {Learn} {In}-{Context}? {A} {Case} {Study} of {Simple} {Function} {Classes}},
  shorttitle = {What {Can} {Transformers} {Learn} {In}-{Context}?},
  url        = {http://arxiv.org/abs/2208.01066},
  doi        = {10.48550/arXiv.2208.01066},
  abstract   = {In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn "most" functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions -- that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the model and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes -- namely sparse linear functions, two-layer neural networks, and decision trees -- with performance that matches or exceeds task-specific learning algorithms. Our code and models are available at https://github.com/dtsip/in-context-learning .},
  urldate    = {2022-09-28},
  publisher  = {arXiv},
  author     = {Garg, Shivam and Tsipras, Dimitris and Liang, Percy and Valiant, Gregory},
  month      = aug,
  year       = {2022},
  note       = {arXiv:2208.01066 [cs]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  file       = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/PFSPLTXR/Garg et al. - 2022 - What Can Transformers Learn In-Context A Case Stu.pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/DFPFILWX/2208.html:text/html}
}

@misc{seo_harp_2022,
  title      = {{HARP}: {Autoregressive} {Latent} {Video} {Prediction} with {High}-{Fidelity} {Image} {Generator}},
  shorttitle = {{HARP}},
  url        = {http://arxiv.org/abs/2209.07143},
  abstract   = {Video prediction is an important yet challenging problem; burdened with the tasks of generating future frames and learning environment dynamics. Recently, autoregressive latent video models have proved to be a powerful video prediction tool, by separating the video prediction into two sub-problems: pre-training an image generator model, followed by learning an autoregressive prediction model in the latent space of the image generator. However, successfully generating high-fidelity and high-resolution videos has yet to be seen. In this work, we investigate how to train an autoregressive latent video prediction model capable of predicting high-fidelity future frames with minimal modification to existing models, and produce high-resolution (256x256) videos. Specifically, we scale up prior models by employing a high-fidelity image generator (VQ-GAN) with a causal transformer model, and introduce additional techniques of top-k sampling and data augmentation to further improve video prediction quality. Despite the simplicity, the proposed method achieves competitive performance to state-of-the-art approaches on standard video prediction benchmarks with fewer parameters, and enables high-resolution video prediction on complex and large-scale datasets. Videos are available at https://sites.google.com/view/harp-videos/home.},
  urldate    = {2022-09-28},
  publisher  = {arXiv},
  author     = {Seo, Younggyo and Lee, Kimin and Liu, Fangchen and James, Stephen and Abbeel, Pieter},
  month      = sep,
  year       = {2022},
  note       = {arXiv:2209.07143 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  annote     = {Comment: Extended draft of the paper accepted to ICIP 2022 conference},
  file       = {arXiv Fulltext PDF:/Users/ethan/Zotero/storage/KHB2PJS3/Seo et al. - 2022 - HARP Autoregressive Latent Video Prediction with .pdf:application/pdf;arXiv.org Snapshot:/Users/ethan/Zotero/storage/TUFDY87H/2209.html:text/html}
}
@article{lake2017building,
  title     = {Building machines that learn and think like people},
  author    = {Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J},
  journal   = {Behavioral and brain sciences},
  volume    = {40},
  pages     = {e253},
  year      = {2017},
  publisher = {Cambridge University Press}
}
@article{kirkpatrick2017overcoming,
  title     = {Overcoming catastrophic forgetting in neural networks},
  author    = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal   = {Proceedings of the national academy of sciences},
  volume    = {114},
  number    = {13},
  pages     = {3521--3526},
  year      = {2017},
  publisher = {National Acad Sciences}
}
@article{french1999catastrophic,
  title     = {Catastrophic forgetting in connectionist networks},
  author    = {French, Robert M},
  journal   = {Trends in cognitive sciences},
  volume    = {3},
  number    = {4},
  pages     = {128--135},
  year      = {1999},
  publisher = {Elsevier}
}
@inproceedings{finn2017model,
  title        = {Model-agnostic meta-learning for fast adaptation of deep networks},
  author       = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle    = {International conference on machine learning},
  pages        = {1126--1135},
  year         = {2017},
  organization = {PMLR}
}
@phdthesis{schmidhuber1987evolutionary,
  title  = {Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook},
  author = {Schmidhuber, J{\"u}rgen},
  year   = {1987},
  school = {Technische Universit{\"a}t M{\"u}nchen}
}
@article{stadie2018some,
  title   = {Some considerations on learning to explore via meta-reinforcement learning},
  author  = {Stadie, Bradly C and Yang, Ge and Houthooft, Rein and Chen, Xi and Duan, Yan and Wu, Yuhuai and Abbeel, Pieter and Sutskever, Ilya},
  journal = {arXiv preprint arXiv:1803.01118},
  year    = {2018}
}
@article{duan2016rl,
  title   = {Rl$^2$: Fast reinforcement learning via slow reinforcement learning},
  author  = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L and Sutskever, Ilya and Abbeel, Pieter},
  journal = {arXiv preprint arXiv:1611.02779},
  year    = {2016}
}
@article{hochreiter1997long,
  title     = {Long short-term memory},
  author    = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal   = {Neural computation},
  volume    = {9},
  number    = {8},
  pages     = {1735--1780},
  year      = {1997},
  publisher = {MIT press}
}
@article{team2023human,
  title   = {Human-Timescale Adaptation in an Open-Ended Task Space},
  author  = {Team, Adaptive Agent and Bauer, Jakob and Baumli, Kate and Baveja, Satinder and Behbahani, Feryal and Bhoopchand, Avishkar and Bradley-Schmieg, Nathalie and Chang, Michael and Clay, Natalie and Collister, Adrian and others},
  journal = {arXiv preprint arXiv:2301.07608},
  year    = {2023}
}
@article{yang2019single,
  title   = {Single episode policy transfer in reinforcement learning},
  author  = {Yang, Jiachen and Petersen, Brenden and Zha, Hongyuan and Faissol, Daniel},
  journal = {arXiv preprint arXiv:1910.07719},
  year    = {2019}
}
@inproceedings{rakelly2019efficient,
  title        = {Efficient off-policy meta-reinforcement learning via probabilistic context variables},
  author       = {Rakelly, Kate and Zhou, Aurick and Finn, Chelsea and Levine, Sergey and Quillen, Deirdre},
  booktitle    = {International conference on machine learning},
  pages        = {5331--5340},
  year         = {2019},
  organization = {PMLR}
}
@inproceedings{pan2022model,
  title        = {Model Predictive Control: A Reinforcement Learning-based Approach},
  author       = {Pan, Xia and Chen, Xiaowei and Zhang, Qingyu and Li, Nannan},
  booktitle    = {Journal of Physics: Conference Series},
  volume       = {2203},
  number       = {1},
  pages        = {012058},
  year         = {2022},
  organization = {IOP Publishing}
}
@article{zintgraf2019varibad,
  title   = {Varibad: A very good method for bayes-adaptive deep rl via meta-learning},
  author  = {Zintgraf, Luisa and Shiarlis, Kyriacos and Igl, Maximilian and Schulze, Sebastian and Gal, Yarin and Hofmann, Katja and Whiteson, Shimon},
  journal = {arXiv preprint arXiv:1910.08348},
  year    = {2019}
}
@inproceedings{fujimoto2019off,
  title        = {Off-policy deep reinforcement learning without exploration},
  author       = {Fujimoto, Scott and Meger, David and Precup, Doina},
  booktitle    = {International conference on machine learning},
  pages        = {2052--2062},
  year         = {2019},
  organization = {PMLR}
}
@article{kumar2020conservative,
  title   = {Conservative q-learning for offline reinforcement learning},
  author  = {Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {1179--1191},
  year    = {2020}
}

@misc{shinn2023reflexion,
  title         = {Reflexion: Language Agents with Verbal Reinforcement Learning},
  author        = {Noah Shinn and Federico Cassano and Edward Berman and Ashwin Gopinath and Karthik Narasimhan and Shunyu Yao},
  year          = {2023},
  eprint        = {2303.11366},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}
@article{madaan2023self,
  title   = {Self-refine: Iterative refinement with self-feedback},
  author  = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal = {arXiv preprint arXiv:2303.17651},
  year    = {2023}
}

@article{tamkin2021understanding,
  title={Understanding the capabilities, limitations, and societal impact of large language models},
  author={Tamkin, Alex and Brundage, Miles and Clark, Jack and Ganguli, Deep},
  journal={arXiv preprint arXiv:2102.02503},
  year={2021}
}
@article{pan2023risk,
  title={On the Risk of Misinformation Pollution with Large Language Models},
  author={Pan, Yikang and Pan, Liangming and Chen, Wenhu and Nakov, Preslav and Kan, Min-Yen and Wang, William Yang},
  journal={arXiv preprint arXiv:2305.13661},
  year={2023}
}
@inproceedings{liang2021towards,
  title={Towards understanding and mitigating social biases in language models},
  author={Liang, Paul Pu and Wu, Chiyu and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle={International Conference on Machine Learning},
  pages={6565--6576},
  year={2021},
  organization={PMLR}
}
@inproceedings{abid2021persistent,
  title={Persistent anti-muslim bias in large language models},
  author={Abid, Abubakar and Farooqi, Maheen and Zou, James},
  booktitle={Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={298--306},
  year={2021}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}


@article{MinigridMiniworld23,
  author       = {Maxime Chevalier-Boisvert and Bolun Dai and Mark Towers and Rodrigo de Lazcano and Lucas Willems and Salem Lahlou and Suman Pal and Pablo Samuel Castro and Jordan Terry},
  title        = {Minigrid \& Miniworld: Modular \& Customizable Reinforcement Learning Environments for Goal-Oriented Tasks},
  journal      = {CoRR},
  volume       = {abs/2306.13831},
  year         = {2023},
}

@inproceedings{janner2021sequence,
  title = {Offline Reinforcement Learning as One Big Sequence Modeling Problem},
  author = {Michael Janner and Qiyang Li and Sergey Levine},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2021},
}

@article{grimm2020value,
  title={The value equivalence principle for model-based reinforcement learning},
  author={Grimm, Christopher and Barreto, Andr{\'e} and Singh, Satinder and Silver, David},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5541--5552},
  year={2020}
}

@article{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@article{silver2021reward,
  title={Reward is enough},
  author={Silver, David and Singh, Satinder and Precup, Doina and Sutton, Richard S},
  journal={Artificial Intelligence},
  volume={299},
  pages={103535},
  year={2021},
  publisher={Elsevier}
}

@article{zheng2018learning,
  title={On learning intrinsic rewards for policy gradient methods},
  author={Zheng, Zeyu and Oh, Junhyuk and Singh, Satinder},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{burda2018exploration,
  title={Exploration by random network distillation},
  author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  journal={arXiv preprint arXiv:1810.12894},
  year={2018}
}

@inproceedings{lecun2016predictive,
  title={Predictive Learning},
  author={LeCun, Yann},
  booktitle={Proceedings of the 30th International Conference on Neural Information Processing Systems (NeurIPS)},
  year={2016},
  address={Barcelona, Spain},
  note={Keynote Speech},
  url={https://neurips.cc/Conferences/2016/Schedule?type=Invited}
}


@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{lieber2021jurassic,
  title={Jurassic-1: Technical details and evaluation},
  author={Lieber, Opher and Sharir, Or and Lenz, Barak and Shoham, Yoav},
  journal={White Paper. AI21 Labs},
  volume={1},
  year={2021}
}

@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}

@article{ramamurthy2022reinforcement,
  title={Is reinforcement learning (not) for natural language processing?: Benchmarks, baselines, and building blocks for natural language policy optimization},
  author={Ramamurthy, Rajkumar and Ammanabrolu, Prithviraj and Brantley, Kiant{\'e} and Hessel, Jack and Sifa, Rafet and Bauckhage, Christian and Hajishirzi, Hannaneh and Choi, Yejin},
  journal={arXiv preprint arXiv:2210.01241},
  year={2022}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{tirumala2022memorization,
  title={Memorization without overfitting: Analyzing the training dynamics of large language models},
  author={Tirumala, Kushal and Markosyan, Aram and Zettlemoyer, Luke and Aghajanyan, Armen},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38274--38290},
  year={2022}
}

@inproceedings{feldman2020does,
  title={Does learning require memorization? a short tale about a long tail},
  author={Feldman, Vitaly},
  booktitle={Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing},
  pages={954--959},
  year={2020}
}

@article{franklin2005elements,
  title={The elements of statistical learning: data mining, inference and prediction},
  author={Franklin, James},
  journal={The Mathematical Intelligencer},
  volume={27},
  number={2},
  pages={83--85},
  year={2005},
  publisher={Springer}
}

@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{brown2021memorization,
  title={When is memorization of irrelevant training data necessary for high-accuracy learning?},
  author={Brown, Gavin and Bun, Mark and Feldman, Vitaly and Smith, Adam and Talwar, Kunal},
  booktitle={Proceedings of the 53rd annual ACM SIGACT symposium on theory of computing},
  pages={123--132},
  year={2021}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@inproceedings{ghazal2013bigbench,
  title={Bigbench: Towards an industry standard benchmark for big data analytics},
  author={Ghazal, Ahmad and Rabl, Tilmann and Hu, Minqing and Raab, Francois and Poess, Meikel and Crolotte, Alain and Jacobsen, Hans-Arno},
  booktitle={Proceedings of the 2013 ACM SIGMOD international conference on Management of data},
  pages={1197--1208},
  year={2013}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{vinyals2019grandmaster,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@article{berner2019dota,
  title={Dota 2 with large scale deep reinforcement learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@misc{fawzi2022discovering,
  title={Discovering novel algorithms with AlphaTensor},
  author={Fawzi, A and Balog, M and Romera-Paredes, B and Hassabis, D and Kohli, P},
  year={2022}
}

@article{hu2020learning,
  title={Learning to utilize shaping rewards: A new approach of reward shaping},
  author={Hu, Yujing and Wang, Weixun and Jia, Hangtian and Wang, Yixiang and Chen, Yingfeng and Hao, Jianye and Wu, Feng and Fan, Changjie},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15931--15941},
  year={2020}
}

@article{andrychowicz2020learning,
  title={Learning dexterous in-hand manipulation},
  author={Andrychowicz, OpenAI: Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and others},
  journal={The International Journal of Robotics Research},
  volume={39},
  number={1},
  pages={3--20},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}